{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 9.3: Model Experiments & Hyperparameter Tuning\n",
    "\n",
    "This notebook experiments with different model architectures and hyperparameters to find optimal configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project to path\n",
    "project_root = Path('..')\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Configure plotting\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Configuration\n",
    "OUTPUT_DIR = Path('outputs/experiments')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_DIR = project_root / 'data' / 'processed'\n",
    "CLASSES = ['No Leak', '1/16\"', '3/32\"', '1/8\"']\n",
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "try:\n",
    "    X_train = np.load(DATA_DIR / 'X_train.npy')\n",
    "    y_train = np.load(DATA_DIR / 'y_train.npy')\n",
    "    X_val = np.load(DATA_DIR / 'X_val.npy')\n",
    "    y_val = np.load(DATA_DIR / 'y_val.npy')\n",
    "    \n",
    "    print(\"✓ Data loaded successfully\")\n",
    "    print(f\"  Training:   {X_train.shape}\")\n",
    "    print(f\"  Validation: {X_val.shape}\")\n",
    "    \n",
    "    # Convert to categorical\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    y_train_cat = to_categorical(y_train, num_classes=4)\n",
    "    y_val_cat = to_categorical(y_val, num_classes=4)\n",
    "    \nexcept Exception as e:\n",
    "    print(f\"⚠ Error loading data: {e}\")\n",
    "    X_train = X_val = y_train = y_val = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Learning Rate Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train is not None:\n",
    "    try:\n",
    "        from tensorflow.keras.models import Sequential\n",
    "        from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D, Dense, Dropout\n",
    "        from tensorflow.keras.optimizers import Adam\n",
    "        \n",
    "        learning_rates = [0.001, 0.005, 0.01, 0.05]\n",
    "        histories = {}\n",
    "        \n",
    "        print(\"\\n=== Learning Rate Experiment ===\")\n",
    "        for lr in learning_rates:\n",
    "            print(f\"\\nTesting lr={lr}...\", end=\" \")\n",
    "            \n",
    "            model = Sequential([\n",
    "                Conv1D(32, 7, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "                Conv1D(64, 5, activation='relu'),\n",
    "                GlobalAveragePooling1D(),\n",
    "                Dense(128, activation='relu'),\n",
    "                Dropout(0.3),\n",
    "                Dense(4, activation='softmax')\n",
    "            ])\n",
    "            \n",
    "            model.compile(\n",
    "                optimizer=Adam(learning_rate=lr),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            hist = model.fit(\n",
    "                X_train, y_train_cat,\n",
    "                validation_data=(X_val, y_val_cat),\n",
    "                epochs=N_EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            histories[f'lr={lr}'] = hist\n",
    "            best_acc = hist.history['val_accuracy'][-1]\n",
    "            print(f\"✓ (best val_acc: {best_acc:.4f})\")\n",
    "        \n",
    "        # Plot results\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        for label, hist in histories.items():\n",
    "            axes[0].plot(hist.history['val_accuracy'], label=label, linewidth=2)\n",
    "            axes[1].plot(hist.history['val_loss'], label=label, linewidth=2)\n",
    "        \n",
    "        axes[0].set_title('Validation Accuracy', fontweight='bold')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[1].set_title('Validation Loss', fontweight='bold')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUTPUT_DIR / 'learning_rate_effects.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"\\n✓ Saved: learning_rate_effects.png\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"⚠ TensorFlow not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Dropout Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train is not None:\n",
    "    try:\n",
    "        from tensorflow.keras.models import Sequential\n",
    "        from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D, Dense, Dropout\n",
    "        from tensorflow.keras.optimizers import Adam\n",
    "        \n",
    "        dropout_rates = [0.0, 0.2, 0.3, 0.4, 0.5]\n",
    "        histories_dropout = {}\n",
    "        \n",
    "        print(\"\\n=== Dropout Rate Experiment ===\")\n",
    "        for dropout in dropout_rates:\n",
    "            print(f\"Testing dropout={dropout}...\", end=\" \")\n",
    "            \n",
    "            model = Sequential([\n",
    "                Conv1D(32, 7, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "                Conv1D(64, 5, activation='relu'),\n",
    "                GlobalAveragePooling1D(),\n",
    "                Dense(128, activation='relu'),\n",
    "                Dropout(dropout),\n",
    "                Dense(4, activation='softmax')\n",
    "            ])\n",
    "            \n",
    "            model.compile(\n",
    "                optimizer=Adam(learning_rate=0.001),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            hist = model.fit(\n",
    "                X_train, y_train_cat,\n",
    "                validation_data=(X_val, y_val_cat),\n",
    "                epochs=N_EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            histories_dropout[f'dropout={dropout}'] = hist\n",
    "            gap = hist.history['accuracy'][-1] - hist.history['val_accuracy'][-1]\n",
    "            print(f\"✓ (gap: {gap:.4f})\")\n",
    "        \n",
    "        # Plot results\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        for label, hist in histories_dropout.items():\n",
    "            axes[0].plot(hist.history['val_accuracy'], label=label, linewidth=2)\n",
    "            gap_trend = np.array(hist.history['accuracy']) - np.array(hist.history['val_accuracy'])\n",
    "            axes[1].plot(gap_trend, label=label, linewidth=2)\n",
    "        \n",
    "        axes[0].set_title('Validation Accuracy', fontweight='bold')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[1].set_title('Train-Val Gap (Overfitting)', fontweight='bold')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Gap')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUTPUT_DIR / 'dropout_effects.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"\\n✓ Saved: dropout_effects.png\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"⚠ TensorFlow not available: {e}\")"
   ]
  },
  {
   "cell_type": {"user": 0}, "metadata": {}, "source": ["## Experiment 3: Model Architecture Comparison"]},
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train is not None:\n",
    "    try:\n",
    "        from tensorflow.keras.models import Sequential\n",
    "        from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D, Dense, Dropout, Flatten, MaxPooling1D\n",
    "        from tensorflow.keras.optimizers import Adam\n",
    "        \n",
    "        architectures = {}\n",
    "        \n",
    "        # Shallow CNN\n",
    "        model = Sequential([\n",
    "            Conv1D(32, 7, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            GlobalAveragePooling1D(),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(4, activation='softmax')\n",
    "        ])\n",
    "        architectures['Shallow'] = model\n",
    "        \n",
    "        # Standard CNN\n",
    "        model = Sequential([\n",
    "            Conv1D(32, 7, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Conv1D(64, 5, activation='relu'),\n",
    "            Conv1D(128, 3, activation='relu'),\n",
    "            GlobalAveragePooling1D(),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(4, activation='softmax')\n",
    "        ])\n",
    "        architectures['Standard'] = model\n",
    "        \n",
    "        # Deep CNN\n",
    "        model = Sequential([\n",
    "            Conv1D(32, 7, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Conv1D(64, 5, activation='relu'),\n",
    "            Conv1D(128, 3, activation='relu'),\n",
    "            Conv1D(256, 3, activation='relu'),\n",
    "            GlobalAveragePooling1D(),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dropout(0.4),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(4, activation='softmax')\n",
    "        ])\n",
    "        architectures['Deep'] = model\n",
    "        \n",
    "        print(\"\\n=== Architecture Comparison ===\")\n",
    "        histories_arch = {}\n",
    "        \n",
    "        for arch_name, model in architectures.items():\n",
    "            print(f\"\\nTraining {arch_name}...\", end=\" \")\n",
    "            \n",
    "            model.compile(\n",
    "                optimizer=Adam(learning_rate=0.001),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            hist = model.fit(\n",
    "                X_train, y_train_cat,\n",
    "                validation_data=(X_val, y_val_cat),\n",
    "                epochs=N_EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            histories_arch[arch_name] = hist\n",
    "            params = model.count_params()\n",
    "            print(f\"✓ ({params:,} params)\")\n",
    "        \n",
    "        # Plot results\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        for arch_name, hist in histories_arch.items():\n",
    "            axes[0].plot(hist.history['val_accuracy'], label=arch_name, linewidth=2)\n",
    "            axes[1].plot(hist.history['val_loss'], label=arch_name, linewidth=2)\n",
    "        \n",
    "        axes[0].set_title('Validation Accuracy', fontweight='bold')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[1].set_title('Validation Loss', fontweight='bold')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUTPUT_DIR / 'architecture_comparison.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"\\n✓ Saved: architecture_comparison.png\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = f\"\"\"\n{\"=\"*60}\nMODEL EXPERIMENTS SUMMARY\n{\"=\"*60}\n\nEXPERIMENT RESULTS:\n\n1. Learning Rate Effects:\n   - Recommended: 0.001 - 0.005\n   - Too low (<0.001): Slow convergence\n   - Too high (>0.01): Unstable training\n\n2. Dropout Rate Effects:\n   - Recommended: 0.3 - 0.4\n   - Effects overfitting but hurts performance if too high\n   - Monitor train-val gap\n\n3. Architecture Comparison:\n   - Shallow: Fast but may underfit\n   - Standard: Good balance\n   - Deep: Better capacity but risk of overfitting\n\nRECOMMENDED CONFIGURATION:\n   Learning Rate: 0.001\n   Dropout Rate: 0.3\n   Architecture: Standard (3 conv layers)\n   Batch Size: 32\n   Epochs: 100 with early stopping\n\nPRINCIPLES:\n   1. Start simple, gradually increase complexity\n   2. Monitor validation metrics continuously\n   3. Use early stopping to prevent overfitting\n   4. Regularize appropriately (dropout, L2)\n   5. Keep learning curves smooth\n\n{\"=\"*60}\n\"\"\"\n\nprint(summary)\n\n# Save summary\nwith open(OUTPUT_DIR / 'experiments_summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\nprint(\"✓ Summary saved: experiments_summary.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL EXPERIMENTS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nOutputs saved to:\", OUTPUT_DIR)\n",
    "print(\"  - learning_rate_effects.png\")\n",
    "print(\"  - dropout_effects.png\")\n",
    "print(\"  - architecture_comparison.png\")\n",
    "print(\"  - experiments_summary.txt\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Train model with recommended configuration\")\n",
    "print(\"  2. Command: python scripts/train_model.py --model-type cnn_1d ...\")\n",
    "print(\"  3. Go to 04_results_analysis.ipynb to evaluate final model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}