{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 9.4: Final Results Analysis & Production Readiness\n",
    "\n",
    "This notebook analyzes the best trained model and provides production recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project to path\n",
    "project_root = Path('..')\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Configure plotting\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Configuration\n",
    "OUTPUT_DIR = Path('outputs/results')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_DIR = project_root / 'data' / 'processed'\n",
    "MODELS_DIR = project_root / 'models'\n",
    "\n",
    "CLASSES = ['No Leak', '1/16\"', '3/32\"', '1/8\"']\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "try:\n",
    "    X_test = np.load(DATA_DIR / 'X_test.npy')\n",
    "    y_test = np.load(DATA_DIR / 'y_test.npy')\n",
    "    print(\"✓ Test data loaded\")\n",
    "    print(f\"  Shape: {X_test.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"⚠ Test data not found\")\n",
    "    X_test = y_test = None\n",
    "\n",
    "# Try to load trained model\n",
    "model = None\n",
    "try:\n",
    "    from tensorflow.keras.models import load_model\n",
    "    \n",
    "    # Look for model files\n",
    "    model_files = list(MODELS_DIR.glob('*.h5')) + list(MODELS_DIR.glob('**/best_model.h5'))\n",
    "    \n",
    "    if model_files:\n",
    "        model_path = model_files[0]\n",
    "        model = load_model(model_path)\n",
    "        print(f\"\\n✓ Model loaded: {model_path.name}\")\n",
    "    else:\n",
    "        print(\"\\n⚠ No trained model found\")\n",
    "        print(\"  Train a model first: python scripts/train_model.py ...\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠ Could not load model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None and X_test is not None:\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score, precision_score, recall_score, f1_score,\n",
    "        confusion_matrix, roc_auc_score, roc_curve, auc, classification_report\n",
    "    )\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict(X_test, verbose=0)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(\"\\n=== Overall Metrics ===\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print(\"\\n=== Per-Class Metrics ===\")\n",
    "    for class_idx, class_name in enumerate(CLASSES):\n",
    "        mask = y_test == class_idx\n",
    "        if mask.sum() > 0:\n",
    "            class_acc = accuracy_score(y_test[mask], y_pred[mask])\n",
    "            print(f\"{class_name:10s}: {class_acc:.4f} ({mask.sum()} samples)\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\n=== Detailed Classification Report ===\")\n",
    "    print(classification_report(y_test, y_pred, target_names=CLASSES, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None and X_test is not None:\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(8, 7))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt='d', cmap='Blues',\n",
    "        xticklabels=CLASSES,\n",
    "        yticklabels=CLASSES,\n",
    "        ax=ax,\n",
    "        cbar_kws={'label': 'Count'}\n",
    "    )\n",
    "    \n",
    "    ax.set_title('Confusion Matrix - Test Set', fontweight='bold', fontsize=14)\n",
    "    ax.set_xlabel('Predicted', fontweight='bold')\n",
    "    ax.set_ylabel('True', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"✓ Saved: confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves (One-vs-Rest)"
   ]
  },
  {
   "cell_type": {"user": 0}, "metadata": {}, "source": []},
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None and X_test is not None:\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    \n",
    "    y_test_bin = label_binarize(y_test, classes=np.arange(len(CLASSES)))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for class_idx, class_name in enumerate(CLASSES):\n",
    "        # Compute ROC curve\n",
    "        fpr, tpr, _ = roc_curve(y_test_bin[:, class_idx], y_pred_proba[:, class_idx])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Plot\n",
    "        ax = axes[class_idx]\n",
    "        ax.plot(fpr, tpr, color='b', lw=2, label=f'ROC (AUC = {roc_auc:.3f})')\n",
    "        ax.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Random')\n",
    "        \n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title(f'{class_name} (One-vs-Rest)', fontweight='bold')\n",
    "        ax.legend(loc='lower right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / 'roc_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"✓ Saved: roc_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": {"user": 0}, "metadata": {}, "source": []},
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None and X_test is not None:\n",
    "    # Identify misclassifications\n",
    "    errors = y_test != y_pred\n",
    "    error_indices = np.where(errors)[0]\n",
    "    \n",
    "    print(f\"\\n=== Error Analysis ===\")\n",
    "    print(f\"Total errors: {errors.sum()} / {len(y_test)} ({100*errors.sum()/len(y_test):.2f}%)\")\n",
    "    \n",
    "    # Error distribution\n",
    "    print(f\"\\nError distribution by true class:\")\n",
    "    for class_idx, class_name in enumerate(CLASSES):\n",
    "        mask = y_test == class_idx\n",
    "        class_errors = errors[mask].sum()\n",
    "        class_total = mask.sum()\n",
    "        if class_total > 0:\n",
    "            print(f\"  {class_name:10s}: {class_errors:3d} / {class_total:3d} ({100*class_errors/class_total:5.2f}%)\")\n",
    "    \n",
    "    # Analyze common confusions\n",
    "    print(f\"\\nCommon misclassification patterns:\")\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    confusions = []\n",
    "    for true_idx in range(len(CLASSES)):\n",
    "        for pred_idx in range(len(CLASSES)):\n",
    "            if true_idx != pred_idx:\n",
    "                count = cm[true_idx, pred_idx]\n",
    "                if count > 0:\n",
    "                    confusions.append((\n",
    "                        count,\n",
    "                        f\"{CLASSES[true_idx]} → {CLASSES[pred_idx]}\"\n",
    "                    ))\n",
    "    \n",
    "    confusions.sort(reverse=True)\n",
    "    for count, pattern in confusions[:5]:\n",
    "        print(f\"  {count:3d}x {pattern}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Analysis"
   ]
  },
  {
   "cell_type": {"user": 0}, "metadata": {}, "source": []},
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None and X_test is not None:\n",
    "    max_proba = y_pred_proba.max(axis=1)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Histogram of confidences\n",
    "    ax = axes[0]\n",
    "    correct = max_proba[y_test == y_pred]\n",
    "    incorrect = max_proba[y_test != y_pred]\n",
    "    \n",
    "    ax.hist(correct, bins=20, alpha=0.6, label='Correct', color='green')\n",
    "    ax.hist(incorrect, bins=20, alpha=0.6, label='Incorrect', color='red')\n",
    "    ax.set_xlabel('Prediction Confidence')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Prediction Confidence Distribution', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy vs confidence threshold\n",
    "    ax = axes[1]\n",
    "    thresholds = np.linspace(0, 1, 101)\n",
    "    accuracies = []\n",
    "    coverage = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        mask = max_proba >= threshold\n",
    "        if mask.sum() > 0:\n",
    "            acc = (y_test[mask] == y_pred[mask]).mean()\n",
    "            cov = mask.sum() / len(y_test)\n",
    "        else:\n",
    "            acc = cov = 0\n",
    "        accuracies.append(acc)\n",
    "        coverage.append(cov)\n",
    "    \n",
    "    ax.plot(thresholds, accuracies, 'b-', linewidth=2, label='Accuracy')\n",
    "    ax.plot(thresholds, coverage, 'r--', linewidth=2, label='Coverage')\n",
    "    ax.set_xlabel('Confidence Threshold')\n",
    "    ax.set_ylabel('Rate')\n",
    "    ax.set_title('Accuracy-Coverage Tradeoff', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / 'confidence_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"✓ Saved: confidence_analysis.png\")\n",
    "    \n",
    "    print(f\"\\n=== Confidence Statistics ===\")\n",
    "    print(f\"Mean confidence (correct):   {correct.mean():.4f}\")\n",
    "    print(f\"Mean confidence (incorrect): {incorrect.mean():.4f}\")\n",
    "    print(f\"\\nRecommended threshold: 0.8\")\n",
    "    mask_08 = max_proba >= 0.8\n",
    "    if mask_08.sum() > 0:\n",
    "        acc_08 = (y_test[mask_08] == y_pred[mask_08]).mean()\n",
    "        cov_08 = mask_08.sum() / len(y_test)\n",
    "        print(f\"  Accuracy at 0.8: {acc_08:.4f}\")\n",
    "        print(f\"  Coverage at 0.8: {cov_08:.4f}\")"
   ]
  },
  {
   "cell_type": {"user": 0}, "metadata": {}, "source": []},
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment Checklist"
   ]
  },
  {
   "cell_type": {"user": 0}, "metadata": {}, "source": []},
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist = f\"\"\"\n{\"=\"*70}\nPRODUCTION DEPLOYMENT CHECKLIST\n{\"=\"*70}\n\nMODEL PERFORMANCE:\n  ✓ Test Accuracy:     {accuracy:.4f}\n  ✓ Precision:         {precision:.4f}\n  ✓ Recall:            {recall:.4f}\n  ✓ F1-Score:          {f1:.4f}\n\nDATA VALIDATION:\n  [ ] Input data shape validation (1024, 9)\n  [ ] Value range checks: [{X_test.min():.4f}, {X_test.max():.4f}]\n  [ ] NaN/Inf detection\n  [ ] Scaling/normalization verification\n\nMODEL OPTIMIZATION:\n  [ ] Convert to TFLite for edge deployment\n  [ ] Quantize model (INT8/FP16)\n  [ ] Test on target hardware\n  [ ] Measure latency: <100ms target\n  [ ] Measure memory: <50MB target\n\nEXPORT & VERSIONING:\n  [ ] Version model checkpoints\n  [ ] Document training parameters\n  [ ] Version training data\n  [ ] Create model card\n\nMONITORING & LOGGING:\n  [ ] Log all predictions\n  [ ] Track prediction confidence\n  [ ] Monitor class distribution drift\n  [ ] Set alerts for accuracy drop\n\nTESTING:\n  [ ] Unit tests for preprocessing\n  [ ] Integration tests for inference\n  [ ] Regression tests on known samples\n  [ ] Edge cases (min/max values, zeros)\n\nDOCUMENTATION:\n  [ ] Document model architecture\n  [ ] List training hyperparameters\n  [ ] Provide usage examples\n  [ ] Include confidence thresholds\n  [ ] Document failure modes\n\nCOMPLIANCE:\n  [ ] Data privacy review\n  [ ] Model explainability assessment\n  [ ] Bias audit across classes\n  [ ] Performance on edge cases\n  [ ] Error handling procedures\n\n{\"=\"*70}\nKEY RECOMMENDATIONS:\n\n1. Confidence Threshold\n   - Recommended: 0.8\n   - Use for high-confidence predictions only\n   - Log low-confidence predictions for review\n\n2. Error Handling\n   - Handle cases where confidence < threshold\n   - Flag uncertain predictions for manual review\n   - Implement fallback procedures\n\n3. Monitoring\n   - Track prediction distribution per class\n   - Monitor accuracy metrics continuously\n   - Alert on significant performance drops\n\n4. Maintenance\n   - Retrain periodically with new data\n   - A/B test model updates\n   - Keep historical models for fallback\n\n{\"=\"*70}\n\"\"\"\n\nprint(checklist)\n\n# Save checklist\nwith open(OUTPUT_DIR / 'deployment_checklist.txt', 'w') as f:\n",
    "    f.write(checklist)\n\nprint(\"✓ Checklist saved: deployment_checklist.txt\")"
   ]
  },
  {
   "cell_type": {"user": 0}, "metadata": {}, "source": []},
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": {"user": 0}, "metadata": {}, "source": []},
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nOutputs saved to:\", OUTPUT_DIR)\n",
    "print(\"  - confusion_matrix.png\")\n",
    "print(\"  - roc_curves.png\")\n",
    "print(\"  - confidence_analysis.png\")\n",
    "print(\"  - deployment_checklist.txt\")\n",
    "print(\"\\nKEY METRICS:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "print(\"\\nNEXT STEPS:\")\n",
    "print(\"  1. Review deployment checklist\")\n",
    "print(\"  2. Export model: python scripts/export_model.py --model-path ...\")\n",
    "print(\"  3. Run benchmarks: python scripts/benchmark.py --model-path ...\")\n",
    "print(\"  4. Deploy to production\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}